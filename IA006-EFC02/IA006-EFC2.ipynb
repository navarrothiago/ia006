{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1 - Classificação binária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.efc2 import csv_reader\n",
    "#%pycat efc2/csv_reader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = csv_reader.load_csv()\n",
    "#data.head(data.shape[0])\n",
    "\n",
    "data = data.drop(\"Unnamed: 0\", 1)\n",
    "data.head(data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(data)\n",
    "\n",
    "plot_rows = int(len(columns) / 2)\n",
    "if(len(columns) % 2 != 0):\n",
    "    plot_rows = plot_rows + 1\n",
    "\n",
    "print(plot_rows)\n",
    "fig1, axs = plt.subplots(plot_rows, 2, constrained_layout=True, figsize=(30,30))\n",
    "\n",
    "data.hist(ax=axs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(30, 30))\n",
    "plt.matshow(data.corr(), fignum=f.number)\n",
    "plt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=45)\n",
    "plt.yticks(range(data.shape[1]), data.columns, fontsize=14)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Correlation Matrix', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(matrix_y, matrix_ye, matrix_phi):\n",
    "    matrix_error = matrix_y - matrix_ye\n",
    "    return -matrix_error.T.dot(matrix_phi)/len(matrix_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x é o phi_x.T\n",
    "def phi(x):\n",
    "    phi1 = np.ones((x.shape[0],1))\n",
    "    return np.concatenate((phi1, x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função custo\n",
    "def j_cross_entropy(targets, predictions, epsilon=1e-12):\n",
    "        \n",
    "    # só funciona se epsilon for maior que zero\n",
    "    assert(epsilon > 0)\n",
    "    \n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "    ce_when_y_1 = -np.sum(targets*np.log(predictions))/N\n",
    "    ce_when_y_0 = - np.sum((1-targets)*np.log(1-predictions))/N\n",
    "    ce = ce_when_y_1 + ce_when_y_0\n",
    "    \n",
    "    #print(\"y = 1 \", ce_when_y_1, \" y = 0\", ce_when_y_0, \" total = \", ce)\n",
    "    \n",
    "    return ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificar se pode estar saturando sigmoide.\n",
    "matrix_phi = phi(preprocessing.scale(train.drop(\"label\", 1).values))\n",
    "\n",
    "# matrix com os dados \n",
    "matrix_y = train[\"label\"].values.reshape(train[\"label\"].values.shape[0], 1)\n",
    "\n",
    "matrix_phi\n",
    "#matrix_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "# Acha o w pelo método grandiente descendente e retorna, também, o custo em cada iteração\n",
    "def find_w(y_train, phi, alpha, iterations):\n",
    "    \n",
    "    # A dimensão da matrix w é número de atributos mais 1 x 1\n",
    "    w = np.random.rand(phi.shape[1], 1)\n",
    "\n",
    "    matrix_cost = np.zeros((iterations,))\n",
    "\n",
    "    for i in range(iterations):\n",
    "        z = phi.dot(w)\n",
    "        matrix_ye = 1 /(1 + np.exp(-z))\n",
    "\n",
    "        w = w - (alpha) * gradient_descent(y_train, matrix_ye, phi).T\n",
    "        matrix_cost[i] = j_cross_entropy(y_train, matrix_ye)\n",
    "        #matrix_cost[i] = log_loss(y_train, matrix_ye)\n",
    "\n",
    "    df_cost = pd.DataFrame(matrix_cost,columns=['cost'])\n",
    "    return w, df_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_w, df_cost = find_w(matrix_y, matrix_phi, 0.01, 10000)\n",
    "df_cost.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase de Teste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificar se pode estar saturando sigmoide.\n",
    "matrix_phi_test = phi(preprocessing.scale(test.drop(\"label\", 1).values))\n",
    "\n",
    "# matrix com os dados de teste\n",
    "# recupera dados com rotulos label e reshape \n",
    "matrix_y_test = test[\"label\"].values.reshape(test[\"label\"].values.shape[0], 1)\n",
    "\n",
    "# calculando estimativa para todos os dados de teste com o w calculado anteriormente\n",
    "z = matrix_phi_test.dot(matrix_w)\n",
    "matrix_ye_test = 1 /(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fase de decisão "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_confusion_df = pd.DataFrame([], columns = ['threshold', 'tp', 'fp', 'tn', 'fn'])\n",
    "\n",
    "# definindo threshold\n",
    "for threshold in np.arange(0, 1.01, 0.1):\n",
    "    #threshold \n",
    "    \n",
    "    # Decisão: coloca (decide por) 1 se for maior, c.c. 0\n",
    "    matrix_ye_test_decided = (matrix_ye_test >= threshold).astype(int)\n",
    "\n",
    "    matrix_confusion = [{'threshold':threshold, 'tp':0, 'tn':0, 'fp':0, 'fn':0}]\n",
    "    row_df = pd.DataFrame(matrix_confusion)\n",
    "\n",
    "    #matrix_y_test.T, matrix_ye_test.T\n",
    "\n",
    "    for y, ye in zip(matrix_y_test, matrix_ye_test_decided):\n",
    "        if(y == ye):\n",
    "            if(y == 1):\n",
    "                row_df[\"tp\"] =  row_df[\"tp\"] + 1\n",
    "            else:\n",
    "                row_df[\"tn\"] =  row_df[\"tn\"] + 1\n",
    "        else:\n",
    "            if(y == 1):\n",
    "                # ye == 0, porem y == 0\n",
    "                row_df[\"fn\"] =  row_df[\"fn\"] + 1\n",
    "            else:\n",
    "                # ye == 1, porem y == 0\n",
    "                row_df[\"fp\"] =  row_df[\"fp\"] + 1\n",
    "                \n",
    "    matrix_confusion_df = matrix_confusion_df.append(row_df, sort=False)\n",
    "\n",
    "matrix_confusion_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC\n",
    "# x - falso positivo (fp / tn + fp ) = (fp / N-) \n",
    "# y - recall - sensibilidade (tp / tp + fn) - true positive\n",
    "pe_ = matrix_confusion_df['fp']/(matrix_confusion_df['tn'] + matrix_confusion_df['fp'])\n",
    "recall = matrix_confusion_df['tp']/(matrix_confusion_df['tp'] + matrix_confusion_df['fn'])\n",
    "\n",
    "plt.plot(pe_, recall, '.--');\n",
    "plt.title(\"Receiver operating curve - ROC\");\n",
    "plt.xlabel(\"false positive - %\");\n",
    "plt.ylabel(\"recall - true positive - %\");\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F-score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proporção de padrões da classe positiva corretamente classificados em \n",
    "# relação a todos os exemplos atribuídos à classe positiva.\n",
    "precisao = matrix_confusion_df['tp']/np.clip(matrix_confusion_df['tp'] + matrix_confusion_df['fp'], 1e-12, None)\n",
    "\n",
    "# Do total de verdadeiro positivo - Proporção de amostras da classe positiva corretamente classificadas. \n",
    "recall = matrix_confusion_df['tp']/np.clip(matrix_confusion_df['tp'] + matrix_confusion_df['fn'], 1e-12, None)\n",
    "\n",
    "pd.DataFrame({\"precisao\": precisao, \"recall\":recall})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f-score\n",
    "\n",
    "# Proporção de padrões da classe positiva corretamente classificados em \n",
    "# relação a todos os exemplos atribuídos à classe positiva.\n",
    "precisao = matrix_confusion_df['tp']/np.clip(matrix_confusion_df['tp'] + matrix_confusion_df['fp'], 1e-12, None)\n",
    "\n",
    "# Do total de verdadeiro positivo - Proporção de amostras da classe positiva corretamente classificadas. \n",
    "recall = matrix_confusion_df['tp']/np.clip(matrix_confusion_df['tp'] + matrix_confusion_df['fn'], 1e-12, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"precisao\": precisao, \"recall\":recall})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround para nao dá divisão por zero. O ideal era tratar esses casos separadamente.\n",
    "recall = np.clip(recall, 1e-12, None)\n",
    "precisao = np.clip(precisao, 1e-12, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1\n",
    "f_score = ((m + 1)*recall*precisao)/(recall + m*precisao)\n",
    "\n",
    "index = np.argmax(np.array(f_score))\n",
    "# Workaround zero divido por zero. O ideal era tratar esse caso antes.\n",
    "ax = plt.plot(matrix_confusion_df['threshold'][:-1], f_score[:-1], '.--');\n",
    "plt.plot(matrix_confusion_df['threshold'].iloc[index], f_score.iloc[index], 'X');\n",
    "plt.annotate(\"Maximum value threshold:\\n\" + str(matrix_confusion_df['threshold'].iloc[index]),\n",
    "            #xy = (matrix_confusion_df['threshold'].iloc[index]/2, f_score.iloc[index]/2))\n",
    "             xy = (0.2, 0.8))\n",
    "plt.title(\"F-score evolution\");\n",
    "plt.xlabel(\"threshold - un\");\n",
    "plt.ylabel(\"F-score - un\");\n",
    "\n",
    "# Para m = 1\n",
    "# Valores  de bem próximos de 1 indicam que o classificador  obteve  \n",
    "# bons resultados tanto na precisão quanto no recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(matrix_confusion_df.iloc[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2 - Classificação multi-classe\n",
    "\n",
    "Técnica adotada: **Um contra todos**\n",
    "\n",
    "A ideia aqui é fazer 5 classificadores, um para cada classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ler Datasets e converte para float cada entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_file = open(\"har_smartphone/X_train.txt\", \"r\")\n",
    "# with open(\"har_smartphone/X_train.txt\", \"r\") as my_file:\n",
    "#   for line in my_file:\n",
    "#       row = [float(x) for x in line.split()]\n",
    "#       print(str)\n",
    "\n",
    "X_train = pd.read_fwf('har_smartphone/X_train.txt', header=None)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_fwf('har_smartphone/X_train.txt', header=None)\n",
    "y_train = pd.read_fwf('har_smartphone/y_train.txt', header=None)\n",
    "X_test = pd.read_fwf('har_smartphone/X_test.txt', header=None)\n",
    "y_test = pd.read_fwf('har_smartphone/y_test.txt', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após separar os dataset de treinamento e teste, iremos achar os w dos 5 classificadores usando a estrutura de regressão logística que minimize o critério da função de custo, _cross-entropy_ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix com os atributos\n",
    "matrix_phi = phi(preprocessing.scale(X_train))\n",
    "\n",
    "# matrix com os dados de validação\n",
    "matrix_y = y_train.values\n",
    "\n",
    "# Número de classificadores e seus parâmetros\n",
    "Q = 6\n",
    "matrix_w = np.zeros((Q,X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para achar os ws, os dados de y devem ser transformados. Como será implementada uma classificação um contra todos, para cada iteração q, sendo q igual ao label do Classificador k, k = 0, ... Q-1:\n",
    "* Se q == k, então o label que identifica a classe k passa a ser 1\n",
    "* Se q != k, então o label que identifica a classe k passa a ser 0\n",
    "\n",
    "(matrix_ye_test >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in range (Q):\n",
    "    # Se for classe q, label -> 1, cc label -> 0\n",
    "    matrix_y_tranform = (matrix_y == q).astype(int)\n",
    "    matrix_w, df_cost = find_w(matrix_y_tranform, matrix_phi, 0.01, 1000)\n",
    "    df_cost.plot();\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
